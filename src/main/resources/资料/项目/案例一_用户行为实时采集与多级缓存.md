

> 说明：
> - Kafka、Redis、HBase需本地或远程启动并可访问。
> - Kafka和Redis依赖见前文。

---

## 1. Kafka Producer 端（模拟用户行为埋点）

```java
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.Properties;
import java.util.Random;

public class UserActionProducer {
    public static void main(String[] args) throws Exception {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        String[] urls = {"/home", "/search", "/cart", "/order", "/profile"};
        Random random = new Random();

        for (int i = 1; i <= 50; i++) {
            String userId = "user" + (random.nextInt(10) + 1);
            String url = urls[random.nextInt(urls.length)];
            long ts = System.currentTimeMillis();
            String msg = String.format("{\"userId\":\"%s\",\"url\":\"%s\",\"ts\":%d}", userId, url, ts);

            producer.send(new ProducerRecord<>("user-action", userId, msg));
            System.out.println("发送消息: " + msg);
            Thread.sleep(500); // 模拟间隔
        }
        producer.close();
    }
}
```

---

## 2. Kafka Consumer 端（写入Redis和HBase）

```java
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.StringDeserializer;
import redis.clients.jedis.Jedis;
import org.json.JSONObject;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;

import java.io.IOException;
import java.time.Duration;
import java.util.*;

public class UserActionConsumer {
    public static void main(String[] args) throws IOException {
        // Kafka Consumer配置
        Properties kafkaProps = new Properties();
        kafkaProps.put("bootstrap.servers", "localhost:9092");
        kafkaProps.put("group.id", "useraction-group");
        kafkaProps.put("key.deserializer", StringDeserializer.class.getName());
        kafkaProps.put("value.deserializer", StringDeserializer.class.getName());
        kafkaProps.put("enable.auto.commit", "true");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(kafkaProps);
        consumer.subscribe(Collections.singletonList("user-action"));

        // Redis连接
        Jedis jedis = new Jedis("localhost", 6379);

        // HBase连接
        Configuration hbaseConf = HBaseConfiguration.create();
        hbaseConf.set("hbase.zookeeper.quorum", "localhost");
        Connection hbaseConn = ConnectionFactory.createConnection(hbaseConf);
        Table table = hbaseConn.getTable(TableName.valueOf("user_action"));

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
            for (ConsumerRecord<String, String> record : records) {
                JSONObject obj = new JSONObject(record.value());
                String userId = obj.getString("userId");
                String url = obj.getString("url");
                long ts = obj.getLong("ts");

                // 1. Redis计数器
                jedis.incr("visit:" + userId);  // PV(page view) UV(user view)

                // 2. HBase存日志
                String rowKey = userId + "_" + ts;
                Put put = new Put(rowKey.getBytes());
                put.addColumn("info".getBytes(), "url".getBytes(), url.getBytes());
                put.addColumn("info".getBytes(), "ts".getBytes(), String.valueOf(ts).getBytes());
                table.put(put);

                System.out.printf("已消费并写入：userId=%s, url=%s, ts=%d%n", userId, url, ts);
            }
        }
        // jedis.close(); table.close(); hbaseConn.close(); // 实际应加finally关闭
    }
}
```

---

## 3. HBase 表建表命令（需提前在hbase shell执行）

```bash
create 'user_action', 'info'
```

---

## 4. 查询Redis计数和HBase历史

**Redis CLI**
```bash
get visit:user3
```

**HBase Shell**
```bash
scan 'user_action'
get 'user_action', 'user3_1688888888888'
```

---

## 5. 依赖（pom.xml）

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>4.0.0</version>
</dependency>
<dependency>
    <groupId>redis.clients</groupId>
    <artifactId>jedis</artifactId>
    <version>4.4.3</version>
</dependency>
<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-client</artifactId>
    <version>2.5.8</version>
</dependency>
<dependency>
    <groupId>org.json</groupId>
    <artifactId>json</artifactId>
    <version>20230618</version>
</dependency>
```
> HBase 需配置 hbase-site.xml，且本地或远程HBase、ZK服务正常。

---

## 6. 说明

- 可以根据实际业务扩展为订单、风控、排行榜等场景。
- 代码结构清晰，便于学生理解消息链路和数据流向。
- 适合作为大数据组件集成入门实训项目。
